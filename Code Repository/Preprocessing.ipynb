{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .output-plaintext, .output-stream, .output{\n",
       "        white-space: pre !important;\n",
       "        font-family: Monaco; # Any monospaced font should work\n",
       "    }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, \n",
    "                               FloatType, TimestampType, DateType, ArrayType, BooleanType)\n",
    "from pyspark.sql.functions import (lit, col, upper, explode, size, array, array_min, array_max, array_except, isnan, row_number, quarter, max)\n",
    "from pyspark.sql import Window\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "#HTML(\"<style>pre { white-space: pre !important; }</style>\")\n",
    "display(HTML(\"\"\"<style>\n",
    "    .output-plaintext, .output-stream, .output{\n",
    "        white-space: pre !important;\n",
    "        font-family: Monaco; # Any monospaced font should work\n",
    "    }</style>\"\"\"))\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory 12G --executor-memory 12G pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 23:50:34 WARN Utils: Your hostname, Prashants-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.38 instead (on interface en0)\n",
      "22/11/01 23:50:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 23:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/01 23:50:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "temp_dir = '/Users/prashantkanth/Desktop/Masters/Semester III/CS543/Project1.nosync/temp'\n",
    "conf = pyspark.SparkConf().setAll([('spark.driver.memory', '30G'), ('spark.executor.memory', '30G'), ('spark.local.dir', temp_dir)])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_data = []\n",
    "raw_path = 'datalake/raw'\n",
    "for updates in os.listdir(raw_path):\n",
    "    if updates != '.DS_Store':\n",
    "        updated_path = os.path.join(raw_path, updates)\n",
    "        for file in os.listdir(updated_path):\n",
    "            openalex_data.append(os.path.join(updated_path, file))\n",
    "print(f'Total files to process: {len(openalex_data)}')\n",
    "# random.shuffle(openalex_data)\n",
    "data_size = 0\n",
    "for file in openalex_data:\n",
    "    data_size += os.path.getsize(file)\n",
    "print(f'Total data_size to process: {round(data_size/(1024*1024*1024),4)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define json schema\n",
    "jsonSchema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('display_name', StringType(), True),\n",
    "    StructField('publication_year', IntegerType(), True),\n",
    "    StructField('publication_date', DateType(), True),\n",
    "    StructField('host_venue', StructType([\n",
    "        StructField('id', StringType(), True),\n",
    "        StructField('issn_l', StringType(), True),\n",
    "        StructField('issn', ArrayType(StringType(), True)),\n",
    "        StructField('display_name', StringType(), True),\n",
    "        StructField('publisher', StringType(), True),\n",
    "        StructField('type', StringType(), True),\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('is_oa', BooleanType(), True),\n",
    "        StructField('version', StringType(), True),\n",
    "        StructField('license', StringType(), True)\n",
    "    ])),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('authorships', ArrayType(StructType([\n",
    "        StructField('author_position', StringType(), True),\n",
    "        StructField('author', StructType([\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('display_name', StringType(), True),\n",
    "            StructField('orcid', StringType(), True)\n",
    "        ])),\n",
    "        StructField('institutions', ArrayType(StructType([\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('display_name', StringType(), True),\n",
    "            StructField('ror', StringType(), True),\n",
    "            StructField('country_code', StringType(), True),\n",
    "            StructField('type', StringType(), True)\n",
    "        ]), True)),\n",
    "        StructField('raw_affiliation_string', StringType(), True)\n",
    "    ]), True)),\n",
    "    StructField('cited_by_count', IntegerType(), True),\n",
    "    StructField('is_retracted', BooleanType(), True),\n",
    "    StructField('concepts', ArrayType(StructType([\n",
    "        StructField('id', StringType(), True),\n",
    "        StructField('wikidata', StringType(), True),\n",
    "        StructField('display_name', StringType(), True),\n",
    "        StructField('level', IntegerType(), True),\n",
    "        StructField('score', FloatType(), True)\n",
    "    ]), True)),\n",
    "    StructField('referenced_works', ArrayType(StringType(), True)),\n",
    "    StructField('related_works', ArrayType(StringType(), True)),\n",
    "    StructField('counts_by_year', ArrayType(StructType([\n",
    "        StructField('year', IntegerType(), True),\n",
    "        StructField('cited_by_count', IntegerType(), True)\n",
    "    ]), True)),\n",
    "    StructField('cited_by_api_url', StringType(), True),\n",
    "    StructField('updated_date', TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json using defined schema\n",
    "works = spark.read.option(\"multiline\", \"true\")\\\n",
    "            .json(openalex_data, schema = jsonSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works.groupBy('publication_year').count().collect()\n",
    "# Drop rows which have publication_year > 2022, this is a known issue with openalex_data ['Questionable Dates'] https://docs.openalex.org/known-issues\n",
    "invalid_rows = works.filter(col('publication_year') > 2022).count()\n",
    "print(f'Inavlid_dates count: {invalid_rows}')\n",
    "\n",
    "# Dropping invalid date rows\n",
    "works = works.filter(col('publication_year') <= 2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing on data\n",
    "cols = ['id', 'display_name', 'publication_year', 'publication_date', 'host_venue', \n",
    "        'type', 'authorships', 'cited_by_count', 'referenced_works', 'counts_by_year']\n",
    "w = Window.partitionBy('id')\n",
    "works = works.withColumn('min_level', array_min(array_except(col('concepts.level'), array(lit(0)))))\\\n",
    "           .select(cols+[explode('concepts').alias('concepts')])\\\n",
    "           .withColumn('concept', col('concepts.display_name'))\\\n",
    "           .withColumn('level', col('concepts.level'))\\\n",
    "           .withColumn('score', col('concepts.score'))\\\n",
    "           .drop('concepts')\\\n",
    "           .filter(col('level') == col('min_level'))\\\n",
    "           .withColumn('maxScore', max('score').over(w))\\\n",
    "           .filter((col('maxScore') > 0.0) & (col('score') == col('maxScore')))\\\n",
    "           .drop('maxScore', 'min_level')\\\n",
    "           .dropDuplicates(['id', 'score'])\\\n",
    "           .drop('level', 'score')\\\n",
    "           .filter((col('type').contains('journal')) | (col('type').contains('proceedings')))\\\n",
    "           .withColumn('quarter', quarter(col('publication_date')))\n",
    "\n",
    "# write to parquet in curated layer\n",
    "curated = 'datalake/curated_new'\n",
    "works.write.partitionBy(\"publication_year\", \"quarter\").parquet(curated)\n",
    "#works.write.partitionBy(partition_on).parquet(curated)\n",
    "print(\"Data written in Parquet format at: {}\".format(curated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- display_name: string (nullable = true)\n",
      " |-- publication_date: date (nullable = true)\n",
      " |-- host_venue: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- issn_l: string (nullable = true)\n",
      " |    |-- issn: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- display_name: string (nullable = true)\n",
      " |    |-- publisher: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- is_oa: boolean (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |    |-- license: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- authorships: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- author_position: string (nullable = true)\n",
      " |    |    |-- author: struct (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |-- orcid: string (nullable = true)\n",
      " |    |    |-- institutions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |-- ror: string (nullable = true)\n",
      " |    |    |    |    |-- country_code: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- raw_affiliation_string: string (nullable = true)\n",
      " |-- cited_by_count: integer (nullable = true)\n",
      " |-- referenced_works: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- counts_by_year: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- cited_by_count: integer (nullable = true)\n",
      " |-- concept: string (nullable = true)\n",
      " |-- publication_year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_works = spark.read.parquet('datalake/curated_new')\n",
    "read_works.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(works.filter(col('publication_year') > 2022).count())\n",
    "works.filter(col('publication_year') <= 2022).groupBy('publication_year').count().collect()\n",
    "#invalid_dates = works.withColumn('year', year(works.publication_date)).filter(col('year')>2022).count()\n",
    "#valid_dates = works.withColumn('year', year(works.publication_date)).filter(col('year')<=2022).groupBy('year').count().collect()\n",
    "#print(f'inavlid count: {invalid_dates}')\n",
    "#valid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorships\n",
    "authorships = works.select('id','authorships')\n",
    "authorships.select('id', explode('authorships').alias('authorships'))\\\n",
    "    .select('id', 'authorships.author_position', 'authorships.author', 'authorships.institutions', 'authorships.raw_affiliation_string').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrievce latest updated_date record for a work id\n",
    "def reducer(x, y):\n",
    "    #X_date = datetime.strptime(x.updated_date, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    #Y_date = datetime.strptime(y.updated_date, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    if x.updated_date > y.updated_date:\n",
    "        return x\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process files in chunks, remove duplicates if any based on updated_date\n",
    "chunks = np.linspace(0, len(openalex_data), len(openalex_data)//6, dtype=int)\n",
    "from_chunk = chunks[:-1]\n",
    "to_chunk = chunks[1:]\n",
    "assert len(from_chunk) == len(to_chunk)\n",
    "for i in tqdm(range(len(chunks)-1),):\n",
    "    # read json file in chunks\n",
    "    works = spark.read.option(\"multiline\",\"true\")\\\n",
    "            .json(openalex_data[from_chunk[i]:to_chunk[i]], schema = jsonSchema)\n",
    "    print(f'row_count[{from_chunk[i]}:{to_chunk[i]}]: {works.count()}')\n",
    "    # get column names of dataframes\n",
    "    cols = works.columns\n",
    "    # retrievce latest updated_date record for a work id\n",
    "    works = works.rdd.map(lambda x: (x.id, x))\\\n",
    "            .reduceByKey(lambda x,y: reducer(x,y))\\\n",
    "            .map(lambda x: x[1])\\\n",
    "            .toDF(jsonSchema)\n",
    "    print(f'After merging dups => row_count[{from_chunk[i]}:{to_chunk[i]}]: {works.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj = datetime.strptime(\"2022-06-11T18:52:04.838486\", '%Y-%m-%dT%H:%M:%S.%f')\n",
    "print(dt_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj2 = datetime.strptime(\"2022-06-12T18:52:04.838486\", '%Y-%m-%dT%H:%M:%S.%f')\n",
    "print(dt_obj2)\n",
    "if dt_obj2 > dt_obj:\n",
    "    print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null/Nan Values\n",
    "works.select(*[\n",
    "    (\n",
    "        count(when(col(c).isNull(), c)) if t in (\"timestamp\", \"date\", \"boolean\") or 'struct' in t\n",
    "        else (count(when(size(col(c))==0, c)) if 'array' in t\n",
    "        else count(when((isnan(c) | col(c).isNull()), c)))\n",
    "    ).alias(c)\n",
    "    for c, t in works.dtypes\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change blank array columns to null\n",
    "for c in works.dtypes:\n",
    "    if \"array\" in c[1]:\n",
    "        works = works.withColumn(c[0], when((size(col(c[0])) == 0), lit(None)).otherwise(col(c[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6deba854f73701cf96f67599d2e89ceb358b45b5aeac6b72dc7e48ba0a56f3a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
